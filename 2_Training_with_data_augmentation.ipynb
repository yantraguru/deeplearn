{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers,models,utils\n",
    "from tensorflow.keras.layers import Dense,Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/caps and shoes/'\n",
    "GEN_DATA_DIR = 'data/caps and shoes generated/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_img_to_data(image):\n",
    "    data = np.asarray(image)\n",
    "    gs_image = image.convert(mode='L')\n",
    "    gs_data = np.asarray(gs_image)\n",
    "    gs_resized = gs_image.resize((112,112))\n",
    "    gs_resized_data = np.asarray(gs_resized)\n",
    "    reshaped_gs_data = gs_resized_data.reshape(112*112)\n",
    "    return reshaped_gs_data\n",
    "\n",
    "def convert_images_from_dir(dir_path):\n",
    "    image_data = []\n",
    "    \n",
    "    for filename in listdir(dir_path):\n",
    "        image = Image.open(dir_path +os.sep + filename)\n",
    "        reshaped_gs_data = convert_img_to_data(image)\n",
    "        image_data.append(reshaped_gs_data)\n",
    "    \n",
    "    return image_data\n",
    "\n",
    "def load_from_dir(dir_path, labels):\n",
    "    label_data = []\n",
    "    image_data = []\n",
    "    for label in labels:\n",
    "        data_from_dir = convert_images_from_dir(dir_path + label)\n",
    "        labels_for_data = [label for i in range(len(data_from_dir))]\n",
    "        image_data += data_from_dir\n",
    "        label_data += labels_for_data\n",
    "    \n",
    "    print('Found %d images belonging to %d classes' % (len(image_data),  len(labels)))\n",
    "    return (np.array(image_data),np.array(label_data))\n",
    "\n",
    "def load_img_data(data_dir):\n",
    "    train_dir = data_dir + 'train/'\n",
    "    validation_dir = data_dir + 'val/'\n",
    "    test_dir = data_dir + 'test/'\n",
    "    \n",
    "    if (os.path.isdir(train_dir) and os.path.isdir(validation_dir) and os.path.isdir(test_dir)) :\n",
    "        labels = [subdirname.name for subdirname in os.scandir(train_dir) if subdirname.is_dir()] \n",
    "        \n",
    "        train_data = load_from_dir(train_dir,labels)\n",
    "        validation_data = load_from_dir(validation_dir,labels)\n",
    "        test_data = load_from_dir(test_dir,labels)\n",
    "        \n",
    "    return train_data, validation_data, test_data     \n",
    "\n",
    "def url_to_image(img_url):\n",
    "    response = requests.get(img_url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img\n",
    "\n",
    "def plot_images(ims, figsize=(24,12), rows=1, interp=False, titles=None):\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=18)\n",
    "        \n",
    "        plt.imshow(ims[i].reshape(112,112), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images belonging to 2 classes\n",
      "Found 100 images belonging to 2 classes\n",
      "Found 50 images belonging to 2 classes\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data = load_img_data(DATA_DIR)\n",
    "X_train, y_train = train_data\n",
    "X_val, y_val = validation_data\n",
    "X_test, y_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "X_val = X_val.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "y_test = le.transform(y_test)\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_val = utils.to_categorical(y_val)\n",
    "y_test = utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_multilayer_model_architecture():\n",
    "    model = models.Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(12544,)))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 308us/sample - loss: 0.5138 - accuracy: 0.7000\n",
      "50/50 [==============================] - 0s 308us/sample - loss: 0.4871 - accuracy: 0.7800\n",
      "50/50 [==============================] - 0s 316us/sample - loss: 0.5133 - accuracy: 0.7200\n",
      "50/50 [==============================] - 0s 419us/sample - loss: 0.5709 - accuracy: 0.6600\n",
      "50/50 [==============================] - 0s 359us/sample - loss: 0.5136 - accuracy: 0.7000\n",
      "Avg training time : 1.323 s\n",
      "Avg test accuracy : 0.7120 +- 0.04\n",
      "Total parameters : 402002\n"
     ]
    }
   ],
   "source": [
    "ITER = 5\n",
    "training_time_list = []\n",
    "test_accuracy_list = []\n",
    "for iter_count in range(ITER):\n",
    "    model = define_multilayer_model_architecture()\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, validation_data = (X_val,y_val), epochs=125, batch_size=200, verbose=0, shuffle=True)\n",
    "    training_time = time.time() - start_time\n",
    "    training_time_list.append(training_time)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=50)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "print('Avg training time : %.3f s' % np.mean(training_time_list))\n",
    "print('Avg test accuracy : %.4f +- %.2f' % (np.mean(test_accuracy_list), np.std(test_accuracy_list)))\n",
    "print('Total parameters : %d' % model.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_singlelayer_model_architecture(no_of_neurons=32):\n",
    "    model = models.Sequential()\n",
    "    model.add(Dense(no_of_neurons, activation='relu', input_shape=(12544,)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 301us/sample - loss: 0.4652 - accuracy: 0.7600\n",
      "50/50 [==============================] - 0s 296us/sample - loss: 0.5087 - accuracy: 0.7400\n",
      "50/50 [==============================] - 0s 288us/sample - loss: 0.4649 - accuracy: 0.7600\n",
      "50/50 [==============================] - 0s 307us/sample - loss: 0.4661 - accuracy: 0.7400\n",
      "50/50 [==============================] - 0s 283us/sample - loss: 0.5386 - accuracy: 0.7000\n",
      "Avg training time : 1.246 s\n",
      "Avg test accuracy : 0.7400 +- 0.02\n",
      "Total parameters : 401506\n"
     ]
    }
   ],
   "source": [
    "ITER = 5\n",
    "training_time_list = []\n",
    "test_accuracy_list = []\n",
    "for iter_count in range(ITER):\n",
    "    model = define_singlelayer_model_architecture(32)\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, validation_data = (X_val,y_val), epochs=125, batch_size=200, verbose=0, shuffle=True)\n",
    "    training_time = time.time() - start_time\n",
    "    training_time_list.append(training_time)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=50)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "print('Avg training time : %.3f s' % np.mean(training_time_list))\n",
    "print('Avg test accuracy : %.4f +- %.2f' % (np.mean(test_accuracy_list), np.std(test_accuracy_list)))\n",
    "print('Total parameters : %d' % model.count_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn from generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes\n",
      "Found 500 images belonging to 2 classes\n",
      "Found 50 images belonging to 2 classes\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data = load_img_data(GEN_DATA_DIR)\n",
    "X_train, y_train = train_data\n",
    "X_val, y_val = validation_data\n",
    "X_test, y_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "X_val = X_val.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_val = le.transform(y_val)\n",
    "y_test = le.transform(y_test)\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_val = utils.to_categorical(y_val)\n",
    "y_test = utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 305us/sample - loss: 0.5509 - accuracy: 0.7000\n",
      "50/50 [==============================] - 0s 343us/sample - loss: 0.5797 - accuracy: 0.6000\n",
      "50/50 [==============================] - 0s 350us/sample - loss: 0.4926 - accuracy: 0.7800\n",
      "50/50 [==============================] - 0s 642us/sample - loss: 0.6063 - accuracy: 0.5400\n",
      "50/50 [==============================] - 0s 379us/sample - loss: 0.5483 - accuracy: 0.6600\n",
      "Avg training time : 3.994 s\n",
      "Avg test accuracy : 0.6560 +- 0.08\n",
      "Total parameters : 402002\n"
     ]
    }
   ],
   "source": [
    "ITER = 5\n",
    "training_time_list = []\n",
    "test_accuracy_list = []\n",
    "for iter_count in range(ITER):\n",
    "    model = define_multilayer_model_architecture()\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, validation_data = (X_val,y_val), epochs=125, batch_size=500, verbose=0, shuffle=True)\n",
    "    training_time = time.time() - start_time\n",
    "    training_time_list.append(training_time)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=50)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "print('Avg training time : %.3f s' % np.mean(training_time_list))\n",
    "print('Avg test accuracy : %.4f +- %.2f' % (np.mean(test_accuracy_list), np.std(test_accuracy_list)))\n",
    "print('Total parameters : %d' % model.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 438us/sample - loss: 0.5551 - accuracy: 0.7400\n",
      "50/50 [==============================] - 0s 478us/sample - loss: 0.5156 - accuracy: 0.8200\n",
      "50/50 [==============================] - 0s 461us/sample - loss: 0.5648 - accuracy: 0.7600\n",
      "50/50 [==============================] - 0s 358us/sample - loss: 0.5837 - accuracy: 0.6400\n",
      "50/50 [==============================] - 0s 400us/sample - loss: 0.5523 - accuracy: 0.7600\n",
      "Avg training time : 4.477 s\n",
      "Avg test accuracy : 0.7440 +- 0.06\n",
      "Total parameters : 401506\n"
     ]
    }
   ],
   "source": [
    "ITER = 5\n",
    "training_time_list = []\n",
    "test_accuracy_list = []\n",
    "for iter_count in range(ITER):\n",
    "    model = define_singlelayer_model_architecture(32)\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, validation_data = (X_val,y_val), epochs=125, batch_size=500, verbose=0, shuffle=True)\n",
    "    training_time = time.time() - start_time\n",
    "    training_time_list.append(training_time)\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, batch_size=50)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "print('Avg training time : %.3f s' % np.mean(training_time_list))\n",
    "print('Avg test accuracy : %.4f +- %.2f' % (np.mean(test_accuracy_list), np.std(test_accuracy_list)))\n",
    "print('Total parameters : %d' % model.count_params())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
